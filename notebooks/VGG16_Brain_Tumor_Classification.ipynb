{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fd784d",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification with VGG16\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for brain tumor classification using VGG16 transfer learning models. It includes data preprocessing, multiple model variants, training, evaluation, and comprehensive result generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Mount Google Drive and set up the environment\n",
    "2. Update the repository\n",
    "3. Install dependencies\n",
    "4. Set up paths to the dataset and results directories\n",
    "5. Load and explore the dataset\n",
    "6. Preprocess the data with augmentation\n",
    "7. Build multiple VGG16 model variants\n",
    "8. Train and validate all models\n",
    "9. Evaluate performance with comprehensive metrics\n",
    "10. Generate visualizations and save all required outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa51b4",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify TensorFlow and GPU availability\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Python version:', platform.python_version())\n",
    "print('GPUs available:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f7554",
   "metadata": {},
   "source": [
    "## 2. Update Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to repository directory and pull updates\n",
    "%cd /content/drive/MyDrive/SE4050-Deep-Learning-Assignment\n",
    "!git pull origin main\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q opencv-python\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "print(\"‚úÖ Repository updated and dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c6d9a",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/SE4050-Deep-Learning-Assignment')\n",
    "\n",
    "from src.common.dataset_utils import create_datasets\n",
    "from src.common.preprocessing import get_augmentation_pipeline, verify_dataset, split_and_copy\n",
    "from src.common.gradcam import generate_gradcam\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f9448",
   "metadata": {},
   "source": [
    "## 4. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = \"/content/drive/MyDrive\"\n",
    "PROJECT_DIR = os.path.join(BASE_DIR, \"SE4050-Deep-Learning-Assignment\")\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, \"BrainTumor\")\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, \"BrainTumor\", \"data\", \"processed\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"BrainTumor\", \"Result\", \"vgg16\")\n",
    "\n",
    "# Model configuration\n",
    "INPUT_SHAPE = (224, 224, 3)  # VGG16 optimal input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR, \"gradcam\"), exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Paths configured:\")\n",
    "print(f\"   Raw data: {RAW_DATA_DIR}\")\n",
    "print(f\"   Processed data: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"   Results: {RESULTS_DIR}\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322a381",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Check if preprocessing is needed and perform data preprocessing if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f864964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed data exists\n",
    "train_dir = os.path.join(PROCESSED_DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(PROCESSED_DATA_DIR, \"val\")\n",
    "test_dir = os.path.join(PROCESSED_DATA_DIR, \"test\")\n",
    "\n",
    "processed_exists, class_folders_valid = verify_dataset(PROCESSED_DATA_DIR)\n",
    "\n",
    "if processed_exists and class_folders_valid:\n",
    "    print(\"‚úÖ Processed data already exists with valid class folders. Skipping preprocessing.\")\n",
    "else:\n",
    "    print(\"üîÑ Starting preprocessing...\")\n",
    "    print(f\"Reading raw images from: {RAW_DATA_DIR}\")\n",
    "    print(f\"Saving processed images to: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "    # Check if raw data exists\n",
    "    yes_dir = os.path.join(RAW_DATA_DIR, \"yes\")\n",
    "    no_dir = os.path.join(RAW_DATA_DIR, \"no\")\n",
    "    \n",
    "    if os.path.exists(RAW_DATA_DIR) and os.path.exists(yes_dir) and os.path.exists(no_dir):\n",
    "        total_files = split_and_copy(RAW_DATA_DIR, PROCESSED_DATA_DIR, [\"yes\", \"no\"])\n",
    "        print(f\"‚úÖ Preprocessing completed successfully! Processed {total_files} images.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not find expected yes/no folders in the raw data directory.\")\n",
    "        print(\"Please make sure your Google Drive contains the correct folder structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ead79",
   "metadata": {},
   "source": [
    "## 6. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with data augmentation for VGG16 (224x224 input)\n",
    "augment = get_augmentation_pipeline()\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(\n",
    "    PROCESSED_DATA_DIR, \n",
    "    BATCH_SIZE, \n",
    "    img_size=(224, 224),  # VGG16 optimal size\n",
    "    augment_fn=augment\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Class names: {class_names}\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE}\")\n",
    "\n",
    "# Display sample images\n",
    "plt.figure(figsize=(12, 8))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(min(9, len(images))):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Class: {class_names[int(labels[i])]}\")\n",
    "        plt.axis('off')\n",
    "plt.suptitle(\"Sample Images from Training Dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"sample_images.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Dataset statistics\n",
    "train_samples = sum(1 for _ in train_ds.unbatch())\n",
    "val_samples = sum(1 for _ in val_ds.unbatch())\n",
    "test_samples = sum(1 for _ in test_ds.unbatch())\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Training samples: {train_samples}\")\n",
    "print(f\"   Validation samples: {val_samples}\")\n",
    "print(f\"   Test samples: {test_samples}\")\n",
    "print(f\"   Total samples: {train_samples + val_samples + test_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd18a2d",
   "metadata": {},
   "source": [
    "## 7. Build VGG16 Model Variants\n",
    "\n",
    "Create multiple VGG16 model variants for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df06874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16_basic(input_shape=INPUT_SHAPE):\n",
    "    \"\"\"\n",
    "    Basic VGG16 transfer learning model - frozen base\n",
    "    \"\"\"\n",
    "    base_model = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze base layers\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_vgg16_fine_tuned(input_shape=INPUT_SHAPE):\n",
    "    \"\"\"\n",
    "    Fine-tuned VGG16 model - unfreeze last few layers\n",
    "    \"\"\"\n",
    "    base_model = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers, unfreeze last block\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),  # Lower LR for fine-tuning\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_vgg16_enhanced(input_shape=INPUT_SHAPE):\n",
    "    \"\"\"\n",
    "    Enhanced VGG16 model with additional regularization\n",
    "    \"\"\"\n",
    "    base_model = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.6),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build all model variants\n",
    "models_dict = {\n",
    "    'VGG16_Basic': build_vgg16_basic(),\n",
    "    'VGG16_Fine_Tuned': build_vgg16_fine_tuned(),\n",
    "    'VGG16_Enhanced': build_vgg16_enhanced()\n",
    "}\n",
    "\n",
    "# Display model summaries\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìã {name} Model Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Save model summary\n",
    "    with open(os.path.join(RESULTS_DIR, f\"{name}_summary.txt\"), \"w\") as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(models_dict)} model variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044d9c3",
   "metadata": {},
   "source": [
    "## 8. Model Training and Validation\n",
    "\n",
    "Train all model variants and track their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, name, epochs=EPOCHS):\n",
    "    \"\"\"Train a model with callbacks and return history\"\"\"\n",
    "    \n",
    "    # Create model-specific directory\n",
    "    model_dir = os.path.join(RESULTS_DIR, name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(model_dir, f\"{name}_best.h5\"),\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=5, \n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüöÄ Training {name}...\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Model directory: {model_dir}\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ {name} training completed!\")\n",
    "    return history\n",
    "\n",
    "# Train all models\n",
    "histories = {}\n",
    "for name, model in models_dict.items():\n",
    "    histories[name] = train_model(model, name)\n",
    "    print(f\"\\n{'-'*50}\\n\")\n",
    "\n",
    "print(\"üéâ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e82cf",
   "metadata": {},
   "source": [
    "## 9. Generate Training Plots\n",
    "\n",
    "Create and save training history plots for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('VGG16 Model Training Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "model_names = list(histories.keys())\n",
    "\n",
    "# Accuracy plots\n",
    "for i, (name, history) in enumerate(histories.items()):\n",
    "    # Training and validation accuracy\n",
    "    axes[0, i].plot(history.history['accuracy'], label='Training', color=colors[i], linewidth=2)\n",
    "    axes[0, i].plot(history.history['val_accuracy'], label='Validation', color=colors[i], linestyle='--', linewidth=2)\n",
    "    axes[0, i].set_title(f'{name} - Accuracy', fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Epoch')\n",
    "    axes[0, i].set_ylabel('Accuracy')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training and validation loss\n",
    "    axes[1, i].plot(history.history['loss'], label='Training', color=colors[i], linewidth=2)\n",
    "    axes[1, i].plot(history.history['val_loss'], label='Validation', color=colors[i], linestyle='--', linewidth=2)\n",
    "    axes[1, i].set_title(f'{name} - Loss', fontweight='bold')\n",
    "    axes[1, i].set_xlabel('Epoch')\n",
    "    axes[1, i].set_ylabel('Loss')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"training_plots_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create individual training plots for each model\n",
    "for name, history in histories.items():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    ax1.set_title(f'{name} - Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2.plot(history.history['loss'], label='Training', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    ax2.set_title(f'{name} - Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{name} Training History', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual plot\n",
    "    model_dir = os.path.join(RESULTS_DIR, name)\n",
    "    plt.savefig(os.path.join(model_dir, f\"{name}_training_plot.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Training plots generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfaa74",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Metrics\n",
    "\n",
    "Evaluate all models and identify the best performer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "def evaluate_model(model, name):\n",
    "    \"\"\"Evaluate model on test set and return metrics\"\"\"\n",
    "    print(f\"\\nüìä Evaluating {name}...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred_proba.extend(predictions.flatten())\n",
    "        y_pred.extend((predictions > 0.5).astype(int).flatten())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    test_precision = precision_score(y_true, y_pred)\n",
    "    test_recall = recall_score(y_true, y_pred)\n",
    "    test_f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Get training metrics\n",
    "    history = histories[name]\n",
    "    train_acc = max(history.history['accuracy'])\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    train_loss = min(history.history['loss'])\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': name,\n",
    "        'train_accuracy': float(train_acc),\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'train_loss': float(train_loss),\n",
    "        'val_loss': float(val_loss),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1_score': float(test_f1),\n",
    "        'epochs_trained': len(history.history['accuracy'])\n",
    "    }\n",
    "    \n",
    "    print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"   Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    return metrics, y_true, y_pred, y_pred_proba\n",
    "\n",
    "# Evaluate all models\n",
    "all_metrics = {}\n",
    "all_predictions = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    # Load best model\n",
    "    model_path = os.path.join(RESULTS_DIR, name, f\"{name}_best.h5\")\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_weights(model_path)\n",
    "        print(f\"‚úÖ Loaded best weights for {name}\")\n",
    "    \n",
    "    metrics, y_true, y_pred, y_pred_proba = evaluate_model(model, name)\n",
    "    all_metrics[name] = metrics\n",
    "    all_predictions[name] = {'y_true': y_true, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba}\n",
    "    \n",
    "    # Save individual model metrics\n",
    "    model_dir = os.path.join(RESULTS_DIR, name)\n",
    "    with open(os.path.join(model_dir, f\"{name}_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics).T\n",
    "print(f\"\\nüìà Model Comparison Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df[['test_accuracy', 'test_precision', 'test_recall', 'test_f1_score']].round(4))\n",
    "\n",
    "# Find best model based on test accuracy\n",
    "best_model_name = comparison_df['test_accuracy'].idxmax()\n",
    "best_model = models_dict[best_model_name]\n",
    "best_metrics = all_metrics[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"   Test F1-Score: {best_metrics['test_f1_score']:.4f}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(os.path.join(RESULTS_DIR, \"model_comparison.csv\"))\n",
    "with open(os.path.join(RESULTS_DIR, \"all_metrics.json\"), \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5739b",
   "metadata": {},
   "source": [
    "## 11. Create Confusion Matrix\n",
    "\n",
    "Generate confusion matrix for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4758906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Confusion Matrices Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (name, predictions) in enumerate(all_predictions.items()):\n",
    "    y_true = predictions['y_true']\n",
    "    y_pred = predictions['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=axes[i], cmap='Blues', values_format='d')\n",
    "    axes[i].set_title(f'{name}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrices_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create detailed confusion matrix for best model\n",
    "best_predictions = all_predictions[best_model_name]\n",
    "y_true_best = best_predictions['y_true']\n",
    "y_pred_best = best_predictions['y_pred']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_best = confusion_matrix(y_true_best, y_pred_best)\n",
    "disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=class_names)\n",
    "disp_best.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_model_name} (Best Model)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report for best model\n",
    "report_best = classification_report(y_true_best, y_pred_best, target_names=class_names)\n",
    "print(f\"\\nüìã Classification Report - {best_model_name}:\")\n",
    "print(\"=\"*60)\n",
    "print(report_best)\n",
    "\n",
    "# Save classification report\n",
    "with open(os.path.join(RESULTS_DIR, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(f\"Classification Report - {best_model_name}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(report_best)\n",
    "\n",
    "print(\"‚úÖ Confusion matrices and classification reports generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ac4d2",
   "metadata": {},
   "source": [
    "## 12. Generate Grad-CAM Visualizations\n",
    "\n",
    "Create Grad-CAM visualizations to understand what the best model focuses on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM visualizations for the best model\n",
    "print(f\"üîç Generating Grad-CAM visualizations for {best_model_name}...\")\n",
    "\n",
    "# Create gradcam directory\n",
    "gradcam_dir = os.path.join(RESULTS_DIR, \"gradcam\")\n",
    "os.makedirs(gradcam_dir, exist_ok=True)\n",
    "\n",
    "# Generate Grad-CAM for best model\n",
    "try:\n",
    "    generate_gradcam(\n",
    "        model=best_model, \n",
    "        dataset=test_ds, \n",
    "        save_dir=gradcam_dir, \n",
    "        class_names=class_names,\n",
    "        num_images=5,  # Generate for 5 images\n",
    "        max_samples=3   # Limit to first 3 batches for speed\n",
    "    )\n",
    "    print(\"‚úÖ Grad-CAM visualizations generated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Grad-CAM generation failed: {str(e)}\")\n",
    "    print(\"Continuing without Grad-CAM...\")\n",
    "\n",
    "# Create a summary visualization showing sample predictions with confidence\n",
    "plt.figure(figsize=(15, 10))\n",
    "sample_count = 0\n",
    "max_samples = 12\n",
    "\n",
    "for images, labels in test_ds.take(2):  # Take 2 batches\n",
    "    predictions = best_model.predict(images, verbose=0)\n",
    "    \n",
    "    for i in range(min(len(images), max_samples - sample_count)):\n",
    "        if sample_count >= max_samples:\n",
    "            break\n",
    "            \n",
    "        plt.subplot(3, 4, sample_count + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        true_label = class_names[int(labels[i])]\n",
    "        pred_prob = predictions[i][0]\n",
    "        pred_label = class_names[1] if pred_prob > 0.5 else class_names[0]\n",
    "        confidence = pred_prob if pred_prob > 0.5 else (1 - pred_prob)\n",
    "        \n",
    "        # Color based on correctness\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        plt.title(f'True: {true_label}\\\\nPred: {pred_label}\\\\nConf: {confidence:.3f}', \n",
    "                 color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "        sample_count += 1\n",
    "    \n",
    "    if sample_count >= max_samples:\n",
    "        break\n",
    "\n",
    "plt.suptitle(f'{best_model_name} - Sample Predictions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"sample_predictions.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample predictions visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351a0dc",
   "metadata": {},
   "source": [
    "## 13. Save Best Model and Final Results\n",
    "\n",
    "Save the best model and generate all required output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd92856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model as best_model.h5\n",
    "best_model_path = os.path.join(RESULTS_DIR, \"best_model.h5\")\n",
    "best_model.save(best_model_path)\n",
    "print(f\"‚úÖ Best model saved: {best_model_path}\")\n",
    "\n",
    "# Save best model's training plot as training_plot.png\n",
    "best_history = histories[best_model_name]\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(best_history.history['accuracy'], label='Training', linewidth=2)\n",
    "plt.plot(best_history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "plt.title(f'{best_model_name} - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(best_history.history['loss'], label='Training', linewidth=2)\n",
    "plt.plot(best_history.history['val_loss'], label='Validation', linewidth=2)\n",
    "plt.title(f'{best_model_name} - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Best Model Training History - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"training_plot.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save metrics.json with best model metrics\n",
    "final_metrics = {\n",
    "    \"best_model\": best_model_name,\n",
    "    \"model_metrics\": best_metrics,\n",
    "    \"training_config\": {\n",
    "        \"input_shape\": INPUT_SHAPE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"class_names\": class_names,\n",
    "        \"train_samples\": train_samples,\n",
    "        \"val_samples\": val_samples,\n",
    "        \"test_samples\": test_samples\n",
    "    },\n",
    "    \"all_models_comparison\": all_metrics\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Final metrics saved: {os.path.join(RESULTS_DIR, 'metrics.json')}\")\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = f\"\"\"\n",
    "# VGG16 Brain Tumor Classification - Results Summary\n",
    "\n",
    "## Best Model: {best_model_name}\n",
    "- **Test Accuracy**: {best_metrics['test_accuracy']:.4f}\n",
    "- **Test Precision**: {best_metrics['test_precision']:.4f}\n",
    "- **Test Recall**: {best_metrics['test_recall']:.4f}\n",
    "- **Test F1-Score**: {best_metrics['test_f1_score']:.4f}\n",
    "\n",
    "## Model Comparison\n",
    "\"\"\"\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "    summary_report += f\"\\\\n### {name}\\\\n\"\n",
    "    summary_report += f\"- Test Accuracy: {metrics['test_accuracy']:.4f}\\\\n\"\n",
    "    summary_report += f\"- Test F1-Score: {metrics['test_f1_score']:.4f}\\\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Generated Files\n",
    "- `best_model.h5` - Best performing model\n",
    "- `training_plot.png` - Training history plots\n",
    "- `confusion_matrix.png` - Confusion matrix\n",
    "- `metrics.json` - Comprehensive metrics\n",
    "- `gradcam/` - Grad-CAM visualizations\n",
    "- `classification_report.txt` - Detailed classification report\n",
    "\n",
    "## Configuration\n",
    "- Input Shape: {INPUT_SHAPE}\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {EPOCHS}\n",
    "- Learning Rate: {LEARNING_RATE}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(RESULTS_DIR, \"summary_report.md\"), \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"‚úÖ Summary report generated!\")\n",
    "\n",
    "# List all generated files\n",
    "print(f\"\\\\nüìÅ Generated files in {RESULTS_DIR}:\")\n",
    "for file in sorted(os.listdir(RESULTS_DIR)):\n",
    "    file_path = os.path.join(RESULTS_DIR, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"   üìÑ {file}\")\n",
    "    elif os.path.isdir(file_path):\n",
    "        print(f\"   üìÅ {file}/\")\n",
    "        for subfile in sorted(os.listdir(file_path)):\n",
    "            print(f\"      üìÑ {subfile}\")\n",
    "\n",
    "print(f\"\\\\nüéâ VGG16 Brain Tumor Classification completed successfully!\")\n",
    "print(f\"üìä Best Model: {best_model_name} with {best_metrics['test_accuracy']:.4f} test accuracy\")\n",
    "print(f\"üìÅ All results saved in: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
