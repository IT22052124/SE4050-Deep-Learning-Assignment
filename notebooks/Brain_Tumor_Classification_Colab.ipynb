{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db3f3a0",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification with CNN\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for brain tumor classification using a CNN model, utilizing the existing project code structure. The dataset should be stored in your Google Drive.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Mount Google Drive and set up the environment\n",
    "2. Clone the repository (if needed)\n",
    "3. Install dependencies\n",
    "4. Set up paths to the dataset and results directories\n",
    "5. Preprocess the data\n",
    "6. Train the CNN model\n",
    "7. Evaluate performance\n",
    "8. Visualize results including Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d10e39",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify TensorFlow and GPU availability\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Python version:', platform.python_version())\n",
    "print('GPUs available:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949479f",
   "metadata": {},
   "source": [
    "## 2. Clone Repository (if needed)\n",
    "\n",
    "If the repository is not already in your Colab environment, clone it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46686dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if repo exists, clone if it doesn't\n",
    "import os\n",
    "\n",
    "repo_name = \"SE4050-Deep-Learning-Assignment\"\n",
    "if not os.path.exists(repo_name):\n",
    "    !git clone https://github.com/IT22052124/SE4050-Deep-Learning-Assignment.git\n",
    "    \n",
    "%cd {repo_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6663f",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79997cd",
   "metadata": {},
   "source": [
    "## 4. Set Up Paths\n",
    "\n",
    "Define paths to your dataset in Google Drive and where to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize these paths according to your Google Drive structure\n",
    "# Define the correct paths based on your folder structure\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "BRAIN_TUMOR_DIR = DRIVE_ROOT + '/BrainTumor'\n",
    "DATA_DIR = BRAIN_TUMOR_DIR + '/data'\n",
    "RAW_DATA_DIR = DATA_DIR + '/archive'  # Contains yes/no folders with raw images\n",
    "PROCESSED_DATA_DIR = DATA_DIR + '/processed'  # Where preprocessed data will be stored\n",
    "RESULTS_DIR = BRAIN_TUMOR_DIR + '/Result/cnn'  # Where model and results will be saved\n",
    "\n",
    "# Create directories if they don't exist\n",
    "!mkdir -p {PROCESSED_DATA_DIR}\n",
    "!mkdir -p {RESULTS_DIR}\n",
    "\n",
    "print(\"Folder structure:\")\n",
    "print(f\"- Brain Tumor Directory: {BRAIN_TUMOR_DIR}\")\n",
    "print(f\"- Raw Data Directory: {RAW_DATA_DIR}\")\n",
    "print(f\"- Processed Data Directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"- Results Directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c841457",
   "metadata": {},
   "source": [
    "## 5. Preprocess the Data\n",
    "\n",
    "Run preprocessing script to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the existence of raw data directories first\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Checking raw data directories...\")\n",
    "print(f\"Looking for raw data in: {RAW_DATA_DIR}\")\n",
    "\n",
    "# First verify the raw data exists and list what's available\n",
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    dirs = [d for d in os.listdir(RAW_DATA_DIR) if os.path.isdir(os.path.join(RAW_DATA_DIR, d))]\n",
    "    print(f\"Found directories: {dirs}\")\n",
    "    \n",
    "    # Check for yes/no directories specifically\n",
    "    yes_dir = os.path.join(RAW_DATA_DIR, \"yes\")\n",
    "    no_dir = os.path.join(RAW_DATA_DIR, \"no\")\n",
    "    \n",
    "    yes_exists = os.path.exists(yes_dir)\n",
    "    no_exists = os.path.exists(no_dir)\n",
    "    \n",
    "    print(f\"'yes' directory exists: {yes_exists}\")\n",
    "    print(f\"'no' directory exists: {no_exists}\")\n",
    "    \n",
    "    # Count files in each directory\n",
    "    if yes_exists:\n",
    "        yes_files = [f for f in os.listdir(yes_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Found {len(yes_files)} image files in 'yes' directory\")\n",
    "        if yes_files:\n",
    "            print(f\"Sample files: {yes_files[:3]}\")\n",
    "    \n",
    "    if no_exists:\n",
    "        no_files = [f for f in os.listdir(no_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Found {len(no_files)} image files in 'no' directory\")\n",
    "        if no_files:\n",
    "            print(f\"Sample files: {no_files[:3]}\")\n",
    "            \n",
    "    # If the directories don't exist, search for alternatives\n",
    "    if not (yes_exists and no_exists):\n",
    "        print(\"\\nSearching for alternative image directories...\")\n",
    "        all_subdirs = []\n",
    "        for root, dirs, _ in os.walk(RAW_DATA_DIR):\n",
    "            for d in dirs:\n",
    "                subdir_path = os.path.join(root, d)\n",
    "                img_count = len([f for f in os.listdir(subdir_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                if img_count > 0:\n",
    "                    all_subdirs.append((subdir_path, img_count))\n",
    "        \n",
    "        if all_subdirs:\n",
    "            print(\"Found alternative directories with images:\")\n",
    "            for path, count in all_subdirs:\n",
    "                print(f\"  - {path}: {count} images\")\n",
    "        else:\n",
    "            print(\"No alternative directories with images found.\")\n",
    "else:\n",
    "    print(f\"❌ Error: Raw data directory {RAW_DATA_DIR} does not exist!\")\n",
    "    print(\"Please make sure your Google Drive contains the correct folder structure.\")\n",
    "    \n",
    "# Now let's define our preprocessing functions\n",
    "# Constants\n",
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "def create_dirs(base_path, classes):\n",
    "    \"\"\"Create directory structure for processed data\"\"\"\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            os.makedirs(os.path.join(base_path, split, cls), exist_ok=True)\n",
    "            \n",
    "def split_and_copy(source_root, dest_root, classes):\n",
    "    \"\"\"Split and copy files into train/val/test directories\"\"\"\n",
    "    random.seed(RANDOM_SEED)\n",
    "    create_dirs(dest_root, classes)\n",
    "    \n",
    "    # Track total files processed\n",
    "    total_processed = 0\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(source_root, cls)\n",
    "        if not os.path.exists(cls_dir):\n",
    "            print(f\"Warning: Class directory {cls_dir} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all image files\n",
    "        imgs = []\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            imgs.extend([os.path.join(cls_dir, f) for f in os.listdir(cls_dir) \n",
    "                         if f.lower().endswith(ext)])\n",
    "            \n",
    "        if not imgs:\n",
    "            print(f\"No images found in {cls_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(imgs)} images in {cls_dir}\")\n",
    "        random.shuffle(imgs)\n",
    "        n = len(imgs)\n",
    "        n_val, n_test = int(n * VAL_SPLIT), int(n * TEST_SPLIT)\n",
    "        n_train = n - n_val - n_test\n",
    "        \n",
    "        splits = {\n",
    "            \"train\": imgs[:n_train],\n",
    "            \"val\": imgs[n_train:n_train+n_val],\n",
    "            \"test\": imgs[n_train+n_val:]\n",
    "        }\n",
    "        \n",
    "        for split, files in splits.items():\n",
    "            print(f\"Processing {cls} -> {split}: {len(files)} images\")\n",
    "            for img_path in tqdm(files):\n",
    "                try:\n",
    "                    # Read and resize image\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Could not read {img_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Resize image\n",
    "                    resized = cv2.resize(img, IMG_SIZE)\n",
    "                    \n",
    "                    # Save to destination\n",
    "                    out_path = os.path.join(dest_root, split, cls, os.path.basename(img_path))\n",
    "                    cv2.imwrite(out_path, resized)\n",
    "                    total_processed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return total_processed\n",
    "\n",
    "print(\"\\nStarting preprocessing...\")\n",
    "print(f\"Reading raw images from: {RAW_DATA_DIR}\")\n",
    "print(f\"Saving processed images to: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "# Process the data\n",
    "if os.path.exists(RAW_DATA_DIR) and (os.path.exists(os.path.join(RAW_DATA_DIR, \"yes\")) or \n",
    "                                     os.path.exists(os.path.join(RAW_DATA_DIR, \"no\"))):\n",
    "    total_files = split_and_copy(RAW_DATA_DIR, PROCESSED_DATA_DIR, [\"yes\", \"no\"])\n",
    "    print(f\"✅ Preprocessing completed successfully! Processed {total_files} images.\")\n",
    "else:\n",
    "    print(\"⚠️ Could not find expected yes/no folders in the raw data directory.\")\n",
    "    print(\"Please make sure your Google Drive contains the correct folder structure,\")\n",
    "    print(\"or manually upload the brain tumor dataset with 'yes' and 'no' subfolders.\")\n",
    "    \n",
    "# Count files in each split to verify\n",
    "print(\"\\nVerifying processed data:\")\n",
    "total_processed_files = 0\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_dir = os.path.join(PROCESSED_DATA_DIR, split)\n",
    "    if os.path.exists(split_dir):\n",
    "        yes_path = os.path.join(split_dir, \"yes\")\n",
    "        no_path = os.path.join(split_dir, \"no\")\n",
    "        \n",
    "        yes_count = len(os.listdir(yes_path)) if os.path.exists(yes_path) else 0\n",
    "        no_count = len(os.listdir(no_path)) if os.path.exists(no_path) else 0\n",
    "        split_total = yes_count + no_count\n",
    "        total_processed_files += split_total\n",
    "        \n",
    "        print(f\"{split.upper()}: yes={yes_count}, no={no_count}, total={split_total}\")\n",
    "\n",
    "print(f\"Total processed images: {total_processed_files}\")\n",
    "\n",
    "# If no files were processed, show a warning\n",
    "if total_processed_files == 0:\n",
    "    print(\"\\n⚠️ WARNING: No images were processed. Training will likely fail.\")\n",
    "    print(\"Please check your Google Drive folder structure and raw data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a6ddf",
   "metadata": {},
   "source": [
    "## 6. Train the CNN Model\n",
    "\n",
    "Run the training script to train the CNN model on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b73ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify that we have processed data to train on\n",
    "import os\n",
    "\n",
    "# Check if processed data exists with the right structure\n",
    "train_dir = os.path.join(PROCESSED_DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(PROCESSED_DATA_DIR, \"val\")\n",
    "test_dir = os.path.join(PROCESSED_DATA_DIR, \"test\")\n",
    "\n",
    "processed_data_exists = (os.path.exists(train_dir) and \n",
    "                         os.path.exists(val_dir) and \n",
    "                         os.path.exists(test_dir))\n",
    "\n",
    "# Check if we have class folders in the train directory\n",
    "class_folders_exist = False\n",
    "if processed_data_exists:\n",
    "    train_yes = os.path.join(train_dir, \"yes\")\n",
    "    train_no = os.path.join(train_dir, \"no\")\n",
    "    \n",
    "    class_folders_exist = (os.path.exists(train_yes) and \n",
    "                           os.path.exists(train_no))\n",
    "    \n",
    "    if class_folders_exist:\n",
    "        yes_count = len(os.listdir(train_yes))\n",
    "        no_count = len(os.listdir(train_no))\n",
    "        \n",
    "        print(f\"Train directory has {yes_count} 'yes' images and {no_count} 'no' images\")\n",
    "        \n",
    "        if yes_count == 0 or no_count == 0:\n",
    "            class_folders_exist = False\n",
    "            print(\"⚠️ One of the class folders is empty!\")\n",
    "\n",
    "# Create a simple modified script to handle the train/val/test structure\n",
    "# We'll create this script on the fly since we don't want to modify your existing code\n",
    "%%writefile /content/modified_train.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project path to sys.path\n",
    "sys.path.append('/content/SE4050-Deep-Learning-Assignment')\n",
    "\n",
    "# Import necessary modules from your project\n",
    "from src.models.cnn.build_cnn import build_cnn_model\n",
    "from src.common.preprocessing import get_augmentation_pipeline\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train CNN for Brain Tumor classification\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, required=True, help=\"Folder containing train/val/test splits\")\n",
    "    parser.add_argument(\"--results_dir\", type=str, required=True, help=\"Folder to write checkpoints and plots\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30)\n",
    "    parser.add_argument(\"--img_size\", type=int, nargs=2, default=(224, 224))\n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_dataset(data_path, img_size=(224, 224), batch_size=32, is_training=False):\n",
    "    \"\"\"Create a TF dataset from a directory with class subdirectories\"\"\"\n",
    "    if is_training:\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    else:\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "    return datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle=is_training\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    os.makedirs(args.results_dir, exist_ok=True)\n",
    "    \n",
    "    # Create datasets from the train/val/test directories\n",
    "    train_dir = os.path.join(args.data_dir, 'train')\n",
    "    val_dir = os.path.join(args.data_dir, 'val')\n",
    "    test_dir = os.path.join(args.data_dir, 'test')\n",
    "    \n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(train_dir) or not os.path.exists(val_dir) or not os.path.exists(test_dir):\n",
    "        print(f\"Error: One or more data directories not found in {args.data_dir}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"Loading data from train: {train_dir}, val: {val_dir}, test: {test_dir}\")\n",
    "    \n",
    "    train_generator = create_dataset(train_dir, img_size=tuple(args.img_size), \n",
    "                                     batch_size=args.batch_size, is_training=True)\n",
    "    val_generator = create_dataset(val_dir, img_size=tuple(args.img_size), \n",
    "                                   batch_size=args.batch_size, is_training=False)\n",
    "    test_generator = create_dataset(test_dir, img_size=tuple(args.img_size), \n",
    "                                    batch_size=args.batch_size, is_training=False)\n",
    "    \n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    print(f\"Class names: {class_names}\")\n",
    "    \n",
    "    # Save class names for evaluation\n",
    "    with open(os.path.join(args.results_dir, \"class_names.json\"), \"w\") as f:\n",
    "        json.dump(class_names, f, indent=2)\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    import numpy as np\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_generator.classes),\n",
    "        y=train_generator.classes\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_cnn_model((*args.img_size, 3))\n",
    "    model.summary()\n",
    "    \n",
    "    # Learning rate schedule: cosine decay\n",
    "    initial_lr = 1e-3\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_lr, decay_steps=args.epochs * 100\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(args.results_dir, \"best_model.keras\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=args.epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict\n",
    "    )\n",
    "    \n",
    "    # Plot and save training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.results_dir, \"history.png\"))\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"val_acc\": float(history.history[\"val_accuracy\"][-1]),\n",
    "        \"train_acc\": float(history.history[\"accuracy\"][-1])\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(args.results_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "        \n",
    "    print(\"✅ Training complete. Best model saved to:\", args.results_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# Choose the right directory for training\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    # If processed data with correct structure exists, use it\n",
    "    TRAIN_DATA_DIR = PROCESSED_DATA_DIR\n",
    "    print(f\"Using processed data for training: {TRAIN_DATA_DIR}\")\n",
    "    print(\"Using our modified script that handles the train/val/test structure\")\n",
    "elif os.path.exists(os.path.join(RAW_DATA_DIR, \"yes\")) and os.path.exists(os.path.join(RAW_DATA_DIR, \"no\")):\n",
    "    # If raw data exists with yes/no folders, use the original script\n",
    "    TRAIN_DATA_DIR = RAW_DATA_DIR\n",
    "    print(f\"Using raw data for training: {TRAIN_DATA_DIR}\")\n",
    "    print(\"Using the original training script\")\n",
    "else:\n",
    "    # Fall back to the original DATA_DIR (parent of archive)\n",
    "    TRAIN_DATA_DIR = DATA_DIR\n",
    "    print(f\"⚠️ Could not find properly structured data. Trying: {TRAIN_DATA_DIR}\")\n",
    "    print(\"Training may fail if the correct data structure is not found.\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the CNN model...\")\n",
    "print(f\"Using data from: {TRAIN_DATA_DIR}\")\n",
    "print(f\"Saving results to: {RESULTS_DIR}\")\n",
    "\n",
    "# Check if we're using the processed data with train/val/test structure\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    # Use our modified script for the train/val/test structure\n",
    "    !python /content/modified_train.py \\\n",
    "        --data_dir {PROCESSED_DATA_DIR} \\\n",
    "        --results_dir {RESULTS_DIR} \\\n",
    "        --epochs 30 \\\n",
    "        --batch_size 32 \\\n",
    "        --img_size 224 224\n",
    "else:\n",
    "    # Use the original script which expects yes/no folders directly\n",
    "    !python -m src.models.cnn.train_cnn \\\n",
    "        --data_dir {TRAIN_DATA_DIR} \\\n",
    "        --results_dir {RESULTS_DIR} \\\n",
    "        --epochs 30 \\\n",
    "        --batch_size 32 \\\n",
    "        --img_size 224 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94332fe3",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model\n",
    "\n",
    "Run the evaluation script to assess model performance on the test set. The script has been enhanced to automatically detect whether it should use the train/val/test folder structure or the original structure with class folders directly under the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041acee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating the CNN model...\")\n",
    "\n",
    "# Choose the right directory for evaluation\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    # If processed data with correct structure exists, use it\n",
    "    EVAL_DATA_DIR = PROCESSED_DATA_DIR\n",
    "    print(f\"Using processed data for evaluation: {EVAL_DATA_DIR}\")\n",
    "else:\n",
    "    # Fall back to the training data directory\n",
    "    EVAL_DATA_DIR = TRAIN_DATA_DIR\n",
    "    print(f\"Using training data directory for evaluation: {EVAL_DATA_DIR}\")\n",
    "\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# Use the enhanced evaluate_cnn script\n",
    "# The script now automatically detects whether to use the train/val/test structure\n",
    "!python -m src.models.cnn.evaluate_cnn \\\n",
    "    --data_dir {EVAL_DATA_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --batch_size 32 \\\n",
    "    --img_size 224 224 \\\n",
    "    --use_processed {1 if processed_data_exists and class_folders_exist else 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67828b",
   "metadata": {},
   "source": [
    "## 8. Display Results\n",
    "\n",
    "Show evaluation metrics, plots, and visualizations stored in the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training history plot\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    history_img = Image.open(f\"{RESULTS_DIR}/history.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(history_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Training History')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying training history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "try:\n",
    "    cm_img = Image.open(f\"{RESULTS_DIR}/confusion_matrix.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying confusion matrix: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2292943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ROC curve\n",
    "try:\n",
    "    roc_img = Image.open(f\"{RESULTS_DIR}/roc_curve.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(roc_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying ROC curve: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881dfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Precision-Recall curve\n",
    "try:\n",
    "    pr_img = Image.open(f\"{RESULTS_DIR}/precision_recall_curve.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(pr_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying Precision-Recall curve: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Grad-CAM visualizations\n",
    "import glob\n",
    "\n",
    "gradcam_images = glob.glob(f\"{RESULTS_DIR}/gradcam/*.png\")\n",
    "if gradcam_images:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, img_path in enumerate(gradcam_images[:5]):\n",
    "        plt.subplot(1, min(5, len(gradcam_images)), i+1)\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Grad-CAM Visualizations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Grad-CAM visualizations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "try:\n",
    "    with open(f\"{RESULTS_DIR}/classification_report.txt\", 'r') as f:\n",
    "        report = f.read()\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading classification report: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open(f\"{RESULTS_DIR}/metrics.json\", 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    print(\"Model Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92072b3",
   "metadata": {},
   "source": [
    "## 9. Make Predictions with the Model\n",
    "\n",
    "Load the best model and make predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Load the best model\n",
    "try:\n",
    "    # Try to load the Keras model first\n",
    "    model_path = f\"{RESULTS_DIR}/best_model.keras\"\n",
    "    if not os.path.exists(model_path):\n",
    "        # Fallback to H5 format\n",
    "        model_path = f\"{RESULTS_DIR}/best_model.h5\"\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    print(f\"Successfully loaded model from {model_path}\")\n",
    "    \n",
    "    # Load class names\n",
    "    try:\n",
    "        with open(f\"{RESULTS_DIR}/class_names.json\", 'r') as f:\n",
    "            class_names = json.load(f)\n",
    "    except:\n",
    "        class_names = [\"no\", \"yes\"]\n",
    "    \n",
    "    print(f\"Classes: {class_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a43020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on sample images\n",
    "def predict_and_display(image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img_display = img.numpy().astype(np.uint8)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(img, verbose=0)[0][0]\n",
    "    predicted_class = class_names[1] if pred > 0.5 else class_names[0]\n",
    "    confidence = pred if pred > 0.5 else 1 - pred\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_display)\n",
    "    plt.title(f\"Prediction: {predicted_class} ({confidence:.2f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Find some sample images to predict - use the processed test images\n",
    "test_yes_dir = os.path.join(PROCESSED_DATA_DIR, \"test\", \"yes\") \n",
    "test_no_dir = os.path.join(PROCESSED_DATA_DIR, \"test\", \"no\")\n",
    "\n",
    "yes_samples = []\n",
    "no_samples = []\n",
    "\n",
    "# Try to get processed test images first\n",
    "if os.path.exists(test_yes_dir):\n",
    "    yes_files = os.listdir(test_yes_dir)\n",
    "    yes_samples = [os.path.join(test_yes_dir, f) for f in yes_files[:2]] if yes_files else []\n",
    "    \n",
    "if os.path.exists(test_no_dir):\n",
    "    no_files = os.listdir(test_no_dir)\n",
    "    no_samples = [os.path.join(test_no_dir, f) for f in no_files[:2]] if no_files else []\n",
    "\n",
    "# If we couldn't find processed test images, try the raw images\n",
    "if not yes_samples and os.path.exists(os.path.join(RAW_DATA_DIR, \"yes\")):\n",
    "    yes_files = os.listdir(os.path.join(RAW_DATA_DIR, \"yes\"))\n",
    "    yes_samples = [os.path.join(RAW_DATA_DIR, \"yes\", f) for f in yes_files[:2]] if yes_files else []\n",
    "    \n",
    "if not no_samples and os.path.exists(os.path.join(RAW_DATA_DIR, \"no\")):\n",
    "    no_files = os.listdir(os.path.join(RAW_DATA_DIR, \"no\"))\n",
    "    no_samples = [os.path.join(RAW_DATA_DIR, \"no\", f) for f in no_files[:2]] if no_files else []\n",
    "\n",
    "sample_images = yes_samples + no_samples\n",
    "\n",
    "if sample_images:\n",
    "    print(f\"Making predictions on {len(sample_images)} sample images\")\n",
    "    for img_path in sample_images:\n",
    "        print(f\"\\nImage: {os.path.basename(img_path)}\")\n",
    "        true_class = \"yes\" if \"yes\" in img_path else \"no\"\n",
    "        pred_class, conf = predict_and_display(img_path)\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted: {pred_class} with {conf:.2f} confidence\")\n",
    "else:\n",
    "    print(\"No sample images found for prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7932c",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook has demonstrated an end-to-end workflow for brain tumor classification using a CNN model:\n",
    "\n",
    "1. We set up the environment and mounted Google Drive\n",
    "2. We prepared the dataset by:\n",
    "   - Reading raw images from `/content/drive/MyDrive/BrainTumor/data/archive`\n",
    "   - Preprocessing them (resize to 224x224)\n",
    "   - Splitting into train/val/test sets (70/15/15%)\n",
    "   - Saving to `/content/drive/MyDrive/BrainTumor/data/processed`\n",
    "3. We trained a CNN model using the processed images\n",
    "4. We evaluated the model and saved results to `/content/drive/MyDrive/BrainTumor/Result/cnn`\n",
    "5. We visualized the results and made predictions on sample images\n",
    "\n",
    "The data flow in this notebook follows your folder structure:\n",
    "- Raw data → Preprocessing → Processed data → Training → Results\n",
    "\n",
    "For further improvements, you could:\n",
    "- Try different CNN architectures like ResNet50 or EfficientNet\n",
    "- Experiment with different preprocessing techniques\n",
    "- Apply more advanced data augmentation\n",
    "- Adjust hyperparameters like learning rate, batch size, etc."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
