{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0c13fd",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification with Optimized CNN\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for brain tumor classification using an optimized CNN model. It includes data preprocessing, model training, and evaluation with proper progress tracking.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Mount Google Drive and set up the environment\n",
    "2. Update the repository\n",
    "3. Install dependencies\n",
    "4. Set up paths to the dataset and results directories\n",
    "5. Preprocess the data\n",
    "6. Train the optimized CNN model\n",
    "7. Evaluate performance\n",
    "8. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12652a",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb31a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify TensorFlow and GPU availability\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Python version:', platform.python_version())\n",
    "print('GPUs available:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc24d60",
   "metadata": {},
   "source": [
    "## 2. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to get the optimized CNN model and improved training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to repository directory and pull updates\n",
    "repo_name = \"SE4050-Deep-Learning-Assignment\"\n",
    "\n",
    "# Check if we're already in the repo directory\n",
    "current_path = os.getcwd()\n",
    "if os.path.basename(current_path) == repo_name:\n",
    "    print(f\"Already in repository directory: {current_path}\")\n",
    "elif os.path.exists(repo_name):\n",
    "    print(f\"Changing to repository directory: {repo_name}\")\n",
    "    %cd {repo_name}\n",
    "else:\n",
    "    print(f\"Repository not found, cloning it...\")\n",
    "    !git clone https://github.com/IT22052124/SE4050-Deep-Learning-Assignment.git\n",
    "    %cd {repo_name}\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull\n",
    "print(\"\\nLatest commit:\")\n",
    "!git log -1 --pretty=format:\"Updated to: %h - %s (%an, %ar)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60f2b5",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01acf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -U pip\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02ee09",
   "metadata": {},
   "source": [
    "## 4. Set Up Paths\n",
    "\n",
    "Define paths to your dataset in Google Drive and where to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize these paths according to your Google Drive structure\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "BRAIN_TUMOR_DIR = DRIVE_ROOT + '/BrainTumor'\n",
    "DATA_DIR = BRAIN_TUMOR_DIR + '/data'\n",
    "RAW_DATA_DIR = DATA_DIR + '/archive'  # Contains yes/no folders with raw images\n",
    "PROCESSED_DATA_DIR = DATA_DIR + '/processed'  # Where preprocessed data will be stored\n",
    "RESULTS_DIR = BRAIN_TUMOR_DIR + '/Result/cnn'  # Where model and results will be saved\n",
    "\n",
    "# Create directories if they don't exist\n",
    "!mkdir -p {PROCESSED_DATA_DIR}\n",
    "!mkdir -p {RESULTS_DIR}\n",
    "\n",
    "print(\"Folder structure:\")\n",
    "print(f\"- Brain Tumor Directory: {BRAIN_TUMOR_DIR}\")\n",
    "print(f\"- Raw Data Directory: {RAW_DATA_DIR}\")\n",
    "print(f\"- Processed Data Directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"- Results Directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee430d6",
   "metadata": {},
   "source": [
    "## 5. Preprocess the Data\n",
    "\n",
    "Run preprocessing script to prepare the dataset. This step creates the train/val/test split from raw images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the existence of raw data directories first\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Checking raw data directories...\")\n",
    "print(f\"Looking for raw data in: {RAW_DATA_DIR}\")\n",
    "\n",
    "# First verify the raw data exists and list what's available\n",
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    dirs = [d for d in os.listdir(RAW_DATA_DIR) if os.path.isdir(os.path.join(RAW_DATA_DIR, d))]\n",
    "    print(f\"Found directories: {dirs}\")\n",
    "    \n",
    "    # Check for yes/no directories specifically\n",
    "    yes_dir = os.path.join(RAW_DATA_DIR, \"yes\")\n",
    "    no_dir = os.path.join(RAW_DATA_DIR, \"no\")\n",
    "    \n",
    "    yes_exists = os.path.exists(yes_dir)\n",
    "    no_exists = os.path.exists(no_dir)\n",
    "    \n",
    "    print(f\"'yes' directory exists: {yes_exists}\")\n",
    "    print(f\"'no' directory exists: {no_exists}\")\n",
    "    \n",
    "    # Count files in each directory\n",
    "    if yes_exists:\n",
    "        yes_files = [f for f in os.listdir(yes_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Found {len(yes_files)} image files in 'yes' directory\")\n",
    "    \n",
    "    if no_exists:\n",
    "        no_files = [f for f in os.listdir(no_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Found {len(no_files)} image files in 'no' directory\")\n",
    "else:\n",
    "    print(f\"❌ Error: Raw data directory {RAW_DATA_DIR} does not exist!\")\n",
    "    print(\"Please make sure your Google Drive contains the correct folder structure.\")\n",
    "    \n",
    "# Now let's define our preprocessing functions\n",
    "# Constants\n",
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "def create_dirs(base_path, classes):\n",
    "    \"\"\"Create directory structure for processed data\"\"\"\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            os.makedirs(os.path.join(base_path, split, cls), exist_ok=True)\n",
    "            \n",
    "def split_and_copy(source_root, dest_root, classes):\n",
    "    \"\"\"Split and copy files into train/val/test directories\"\"\"\n",
    "    random.seed(RANDOM_SEED)\n",
    "    create_dirs(dest_root, classes)\n",
    "    \n",
    "    # Track total files processed\n",
    "    total_processed = 0\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(source_root, cls)\n",
    "        if not os.path.exists(cls_dir):\n",
    "            print(f\"Warning: Class directory {cls_dir} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all image files\n",
    "        imgs = []\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            imgs.extend([os.path.join(cls_dir, f) for f in os.listdir(cls_dir) \n",
    "                         if f.lower().endswith(ext)])\n",
    "            \n",
    "        if not imgs:\n",
    "            print(f\"No images found in {cls_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(imgs)} images in {cls_dir}\")\n",
    "        random.shuffle(imgs)\n",
    "        n = len(imgs)\n",
    "        n_val, n_test = int(n * VAL_SPLIT), int(n * TEST_SPLIT)\n",
    "        n_train = n - n_val - n_test\n",
    "        \n",
    "        splits = {\n",
    "            \"train\": imgs[:n_train],\n",
    "            \"val\": imgs[n_train:n_train+n_val],\n",
    "            \"test\": imgs[n_train+n_val:]\n",
    "        }\n",
    "        \n",
    "        for split, files in splits.items():\n",
    "            print(f\"Processing {cls} -> {split}: {len(files)} images\")\n",
    "            for img_path in tqdm(files):\n",
    "                try:\n",
    "                    # Read and resize image\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Could not read {img_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Resize image\n",
    "                    resized = cv2.resize(img, IMG_SIZE)\n",
    "                    \n",
    "                    # Save to destination\n",
    "                    out_path = os.path.join(dest_root, split, cls, os.path.basename(img_path))\n",
    "                    cv2.imwrite(out_path, resized)\n",
    "                    total_processed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return total_processed\n",
    "\n",
    "# Check if preprocessing is needed (if processed data doesn't already exist)\n",
    "train_dir = os.path.join(PROCESSED_DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(PROCESSED_DATA_DIR, \"val\")\n",
    "test_dir = os.path.join(PROCESSED_DATA_DIR, \"test\")\n",
    "\n",
    "processed_exists = (os.path.exists(train_dir) and \n",
    "                   os.path.exists(val_dir) and \n",
    "                   os.path.exists(test_dir))\n",
    "\n",
    "if processed_exists:\n",
    "    print(\"\\nProcessed data already exists. Skipping preprocessing.\")\n",
    "    # Check count of images in each split\n",
    "    train_yes = len(os.listdir(os.path.join(train_dir, \"yes\"))) if os.path.exists(os.path.join(train_dir, \"yes\")) else 0\n",
    "    train_no = len(os.listdir(os.path.join(train_dir, \"no\"))) if os.path.exists(os.path.join(train_dir, \"no\")) else 0\n",
    "    val_yes = len(os.listdir(os.path.join(val_dir, \"yes\"))) if os.path.exists(os.path.join(val_dir, \"yes\")) else 0\n",
    "    val_no = len(os.listdir(os.path.join(val_dir, \"no\"))) if os.path.exists(os.path.join(val_dir, \"no\")) else 0\n",
    "    test_yes = len(os.listdir(os.path.join(test_dir, \"yes\"))) if os.path.exists(os.path.join(test_dir, \"yes\")) else 0\n",
    "    test_no = len(os.listdir(os.path.join(test_dir, \"no\"))) if os.path.exists(os.path.join(test_dir, \"no\")) else 0\n",
    "    \n",
    "    print(f\"Train: {train_yes} yes, {train_no} no\")\n",
    "    print(f\"Validation: {val_yes} yes, {val_no} no\")\n",
    "    print(f\"Test: {test_yes} yes, {test_no} no\")\n",
    "else:\n",
    "    print(\"\\nStarting preprocessing...\")\n",
    "    print(f\"Reading raw images from: {RAW_DATA_DIR}\")\n",
    "    print(f\"Saving processed images to: {PROCESSED_DATA_DIR}\")\n",
    "\n",
    "    # Process the data\n",
    "    if os.path.exists(RAW_DATA_DIR) and (os.path.exists(yes_dir) and os.path.exists(no_dir)):\n",
    "        total_files = split_and_copy(RAW_DATA_DIR, PROCESSED_DATA_DIR, [\"yes\", \"no\"])\n",
    "        print(f\"✅ Preprocessing completed successfully! Processed {total_files} images.\")\n",
    "    else:\n",
    "        print(\"⚠️ Could not find expected yes/no folders in the raw data directory.\")\n",
    "        print(\"Please make sure your Google Drive contains the correct folder structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f2e1d",
   "metadata": {},
   "source": [
    "## 6. Train the Optimized CNN Model\n",
    "\n",
    "Run the training script to train the CNN model on the preprocessed data. The optimized CNN has a better architecture with fewer parameters and proper progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify that we have processed data to train on\n",
    "import os\n",
    "\n",
    "# Check if processed data exists with the right structure\n",
    "train_dir = os.path.join(PROCESSED_DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(PROCESSED_DATA_DIR, \"val\")\n",
    "test_dir = os.path.join(PROCESSED_DATA_DIR, \"test\")\n",
    "\n",
    "processed_data_exists = (os.path.exists(train_dir) and \n",
    "                         os.path.exists(val_dir) and \n",
    "                         os.path.exists(test_dir))\n",
    "\n",
    "# Check if we have class folders in the train directory\n",
    "class_folders_exist = False\n",
    "if processed_data_exists:\n",
    "    train_yes = os.path.join(train_dir, \"yes\")\n",
    "    train_no = os.path.join(train_dir, \"no\")\n",
    "    \n",
    "    class_folders_exist = (os.path.exists(train_yes) and \n",
    "                           os.path.exists(train_no))\n",
    "    \n",
    "    if class_folders_exist:\n",
    "        yes_count = len(os.listdir(train_yes))\n",
    "        no_count = len(os.listdir(train_no))\n",
    "        \n",
    "        print(f\"Train directory has {yes_count} 'yes' images and {no_count} 'no' images\")\n",
    "        \n",
    "        if yes_count == 0 or no_count == 0:\n",
    "            class_folders_exist = False\n",
    "            print(\"⚠️ One of the class folders is empty!\")\n",
    "\n",
    "# Choose the right directory for training\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    # If processed data with correct structure exists, use it\n",
    "    TRAIN_DATA_DIR = PROCESSED_DATA_DIR\n",
    "    print(f\"Using processed data for training: {TRAIN_DATA_DIR}\")\n",
    "    print(\"The script will automatically detect the train/val/test structure\")\n",
    "elif os.path.exists(os.path.join(RAW_DATA_DIR, \"yes\")) and os.path.exists(os.path.join(RAW_DATA_DIR, \"no\")):\n",
    "    # If raw data exists with yes/no folders, use the original data structure\n",
    "    TRAIN_DATA_DIR = RAW_DATA_DIR\n",
    "    print(f\"Using raw data for training: {TRAIN_DATA_DIR}\")\n",
    "    print(\"The script will use the original data structure with class folders\")\n",
    "else:\n",
    "    # Fall back to the original DATA_DIR (parent of archive)\n",
    "    TRAIN_DATA_DIR = DATA_DIR\n",
    "    print(f\"⚠️ Could not find properly structured data. Trying: {TRAIN_DATA_DIR}\")\n",
    "    print(\"Training may fail if the correct data structure is not found.\")\n",
    "\n",
    "# Calculate training parameters\n",
    "batch_size = 32\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    train_yes = os.path.join(train_dir, \"yes\")\n",
    "    train_no = os.path.join(train_dir, \"no\")\n",
    "    total_train_images = len(os.listdir(train_yes)) + len(os.listdir(train_no))\n",
    "    steps_per_epoch = total_train_images // batch_size\n",
    "    print(f\"Total training images: {total_train_images}\")\n",
    "    print(f\"With batch size {batch_size}, steps_per_epoch should be: {steps_per_epoch}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the CNN model...\")\n",
    "print(f\"Using data from: {TRAIN_DATA_DIR}\")\n",
    "print(f\"Saving results to: {RESULTS_DIR}\")\n",
    "\n",
    "# Use the enhanced train_cnn.py script which now handles both data structures\n",
    "# and properly calculates steps_per_epoch\n",
    "!python -m src.models.cnn.train_cnn \\\n",
    "    --data_dir {TRAIN_DATA_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --epochs 20 \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --img_size 224 224 \\\n",
    "    --use_processed {1 if processed_data_exists and class_folders_exist else 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7c715",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model\n",
    "\n",
    "Run the evaluation script to assess model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating the CNN model...\")\n",
    "\n",
    "# Choose the right directory for evaluation\n",
    "if processed_data_exists and class_folders_exist:\n",
    "    # If processed data with correct structure exists, use it\n",
    "    EVAL_DATA_DIR = PROCESSED_DATA_DIR\n",
    "    print(f\"Using processed data for evaluation: {EVAL_DATA_DIR}\")\n",
    "else:\n",
    "    # Fall back to the training data directory\n",
    "    EVAL_DATA_DIR = TRAIN_DATA_DIR\n",
    "    print(f\"Using training data directory for evaluation: {EVAL_DATA_DIR}\")\n",
    "\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# Use the enhanced evaluate_cnn script\n",
    "# The script now automatically detects whether to use the train/val/test structure\n",
    "!python -m src.models.cnn.evaluate_cnn \\\n",
    "    --data_dir {EVAL_DATA_DIR} \\\n",
    "    --results_dir {RESULTS_DIR} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --img_size 224 224 \\\n",
    "    --use_processed {1 if processed_data_exists and class_folders_exist else 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb698c3",
   "metadata": {},
   "source": [
    "## 8. Display Results\n",
    "\n",
    "Show evaluation metrics, plots, and visualizations stored in the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training history plot\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    history_img = Image.open(f\"{RESULTS_DIR}/history.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(history_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Training History')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying training history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "try:\n",
    "    cm_img = Image.open(f\"{RESULTS_DIR}/confusion_matrix.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying confusion matrix: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ROC curve and Precision-Recall curve if available\n",
    "try:\n",
    "    roc_img = Image.open(f\"{RESULTS_DIR}/roc_curve.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(roc_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not display ROC curve: {e}\")\n",
    "\n",
    "try:\n",
    "    pr_img = Image.open(f\"{RESULTS_DIR}/precision_recall_curve.png\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(pr_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not display Precision-Recall curve: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f19116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "try:\n",
    "    with open(f\"{RESULTS_DIR}/classification_report.txt\", 'r') as f:\n",
    "        report = f.read()\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading classification report: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open(f\"{RESULTS_DIR}/metrics.json\", 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    print(\"Model Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ac305",
   "metadata": {},
   "source": [
    "## 9. Make Predictions with the Model\n",
    "\n",
    "Load the best model and make predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Load the best model\n",
    "try:\n",
    "    # Try to load the Keras model first\n",
    "    model_path = f\"{RESULTS_DIR}/best_model.keras\"\n",
    "    if not os.path.exists(model_path):\n",
    "        # Fallback to H5 format\n",
    "        model_path = f\"{RESULTS_DIR}/best_model.h5\"\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    print(f\"Successfully loaded model from {model_path}\")\n",
    "    \n",
    "    # Load class names\n",
    "    try:\n",
    "        with open(f\"{RESULTS_DIR}/class_names.json\", 'r') as f:\n",
    "            class_names = json.load(f)\n",
    "    except:\n",
    "        class_names = [\"no\", \"yes\"]\n",
    "    \n",
    "    print(f\"Classes: {class_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3177a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on sample images\n",
    "def predict_and_display(image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img_display = img.numpy().astype(np.uint8)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(img, verbose=0)[0][0]\n",
    "    predicted_class = class_names[1] if pred > 0.5 else class_names[0]\n",
    "    confidence = pred if pred > 0.5 else 1 - pred\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_display)\n",
    "    plt.title(f\"Prediction: {predicted_class} ({confidence:.2f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Find some sample images to predict - use the processed test images\n",
    "test_yes_dir = os.path.join(PROCESSED_DATA_DIR, \"test\", \"yes\") \n",
    "test_no_dir = os.path.join(PROCESSED_DATA_DIR, \"test\", \"no\")\n",
    "\n",
    "yes_samples = []\n",
    "no_samples = []\n",
    "\n",
    "# Try to get processed test images first\n",
    "if os.path.exists(test_yes_dir):\n",
    "    yes_files = os.listdir(test_yes_dir)\n",
    "    yes_samples = [os.path.join(test_yes_dir, f) for f in yes_files[:2]] if yes_files else []\n",
    "    \n",
    "if os.path.exists(test_no_dir):\n",
    "    no_files = os.listdir(test_no_dir)\n",
    "    no_samples = [os.path.join(test_no_dir, f) for f in no_files[:2]] if no_files else []\n",
    "\n",
    "# If we couldn't find processed test images, try the raw images\n",
    "if not yes_samples and os.path.exists(os.path.join(RAW_DATA_DIR, \"yes\")):\n",
    "    yes_files = os.listdir(os.path.join(RAW_DATA_DIR, \"yes\"))\n",
    "    yes_samples = [os.path.join(RAW_DATA_DIR, \"yes\", f) for f in yes_files[:2]] if yes_files else []\n",
    "    \n",
    "if not no_samples and os.path.exists(os.path.join(RAW_DATA_DIR, \"no\")):\n",
    "    no_files = os.listdir(os.path.join(RAW_DATA_DIR, \"no\"))\n",
    "    no_samples = [os.path.join(RAW_DATA_DIR, \"no\", f) for f in no_files[:2]] if no_files else []\n",
    "\n",
    "sample_images = yes_samples + no_samples\n",
    "\n",
    "if sample_images:\n",
    "    print(f\"Making predictions on {len(sample_images)} sample images\")\n",
    "    for img_path in sample_images:\n",
    "        print(f\"\\nImage: {os.path.basename(img_path)}\")\n",
    "        true_class = \"yes\" if \"yes\" in img_path else \"no\"\n",
    "        pred_class, conf = predict_and_display(img_path)\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Predicted: {pred_class} with {conf:.2f} confidence\")\n",
    "else:\n",
    "    print(\"No sample images found for prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94556644",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook has demonstrated an end-to-end workflow for brain tumor classification using an optimized CNN model with the following advantages:\n",
    "\n",
    "1. **Optimized Architecture**:\n",
    "   - Reduced parameters: ~1.2 million vs 25.7 million in the original model\n",
    "   - Smaller memory footprint: 4.55 MB vs 98.36 MB\n",
    "   - Faster training time\n",
    "\n",
    "2. **Proper Progress Tracking**:\n",
    "   - Calculates steps_per_epoch properly for accurate progress bars\n",
    "   - Shows exact training progress instead of \"Unknown\"\n",
    "\n",
    "3. **Better Resource Utilization**:\n",
    "   - More efficient memory usage\n",
    "   - Faster inference time\n",
    "\n",
    "For further improvements, you could:\n",
    "- Try different CNN architectures like ResNet50 or EfficientNet\n",
    "- Experiment with different preprocessing techniques\n",
    "- Apply more advanced data augmentation\n",
    "- Adjust hyperparameters like learning rate, batch size, etc."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
